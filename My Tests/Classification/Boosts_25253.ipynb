{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier,StackingClassifier,BaggingClassifier,AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer,mean_squared_error\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILES READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/Users/talalkhan/Documents/Data Sets/Second Challange/train.csv\")\n",
    "df2 = pd.read_csv(\"/Users/talalkhan/Documents/Data Sets/Second Challange/test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do onehot encoding for categorical columns\n",
    "df1 = pd.get_dummies(df1)\n",
    "df2 = pd.get_dummies(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HighScore OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (X) and target variable (y) for training data\n",
    "X_train = df1.drop('price_doc', axis=1)\n",
    "y_train = df1['price_doc']\n",
    "\n",
    "# Separate the features (X) and target variable (y) for test data\n",
    "X_test = df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "\n",
    "# Perform one-hot encoding for categorical columns for training data\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded_train = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names_encoded = encoder.get_feature_names_out(categorical_cols)\n",
    "X_encoded_train_df = pd.DataFrame(X_encoded_train, columns=feature_names_encoded)\n",
    "X_train.drop(categorical_cols, axis=1, inplace=True)\n",
    "X_train = pd.concat([X_train, X_encoded_train_df], axis=1)\n",
    "\n",
    "# Perform one-hot encoding for categorical columns for test data\n",
    "X_encoded_test = encoder.transform(X_test[categorical_cols])\n",
    "X_encoded_test_df = pd.DataFrame(X_encoded_test, columns=feature_names_encoded)\n",
    "X_test.drop(categorical_cols, axis=1, inplace=True)\n",
    "X_test = pd.concat([X_test, X_encoded_test_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to handle categorical data\n",
    "'''# List of categorical columns to label encode\n",
    "categorical_columns = ['ethnicity', 'gender', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']\n",
    "\n",
    "# Initialize a LabelEncoder for each categorical column\n",
    "label_encoders = {}\n",
    "#label_encoders2 = {}\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df1[column] = le.fit_transform(df1[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le2 = LabelEncoder()\n",
    "    df2[column] = le2.fit_transform(df2[column])\n",
    "    label_encoders[column] = le2'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping low correlation columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop colums with low correlation RecordID,encounter_id,patient_id,hospital_id,icu_id\n",
    "df1 = df1.drop(['ventilated_apache','apache_4a_hospital_death_prob','icu_stay_type_readmit', 'apache_3j_bodysystem_Gynecological', 'apache_2_bodysystem_Undefined Diagnoses'], axis=1)\n",
    "df2 = df2.drop(['ventilated_apache', 'apache_4a_hospital_death_prob','icu_stay_type_readmit', 'apache_3j_bodysystem_Gynecological', 'apache_2_bodysystem_Undefined Diagnoses'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#using KNNImputer to handle nan values\n",
    "#df1\n",
    "imr = KNNImputer(n_neighbors=2500, weights='uniform')\n",
    "imr = imr.fit(df1.values)\n",
    "imputed_data1 = imr.transform(df1.values)\n",
    "#df2\n",
    "imr = KNNImputer(n_neighbors=2500, weights='uniform')\n",
    "imr = imr.fit(df2.values)\n",
    "imputed_data2 = imr.transform(df2.values)\n",
    "'''\n",
    "\n",
    "#'''\n",
    "#using simpleimputer to handle nan values\n",
    "#df1\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(df1.values)\n",
    "imputed_data1 = imr.transform(df1.values)\n",
    "#df2\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(df2.values)\n",
    "imputed_data2 = imr.transform(df2.values)\n",
    "#'''\n",
    "\n",
    "# convert the imputed NumPy array back into a Pandas DataFrame\n",
    "df1 = pd.DataFrame(imputed_data1, columns=df1.columns)\n",
    "df2 = pd.DataFrame(imputed_data2, columns=df2.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling / Other minmaxing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMaxscaling \n",
    "scaler = MinMaxScaler()\n",
    "cols_to_scale = df1.columns[df1.columns != 'RecordID']\n",
    "temp = df1.loc[:, ['RecordID']]\n",
    "df1 = pd.DataFrame(scaler.fit_transform(df1[cols_to_scale]), columns=scaler.get_feature_names_out())\n",
    "df1 = pd.concat([temp, df1], axis=1, join='inner')\n",
    "\n",
    "cols_to_scale = df2.columns[df2.columns != 'RecordID']\n",
    "temp = df2.loc[:, ['RecordID']]\n",
    "df2 = pd.DataFrame(scaler.fit_transform(df2[cols_to_scale]), columns=scaler.get_feature_names_out())\n",
    "df2 = pd.concat([temp, df2], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rohbust scaling \n",
    "scaler = RobustScaler()\n",
    "cols_to_scale = df1.columns[df1.columns != 'RecordID']\n",
    "temp = df1.loc[:, ['RecordID']]\n",
    "df1 = pd.DataFrame(scaler.fit_transform(df1[cols_to_scale]), columns=scaler.get_feature_names_out())\n",
    "df1 = pd.concat([temp, df1], axis=1, join='inner')\n",
    "\n",
    "cols_to_scale = df2.columns[df2.columns != 'RecordID']\n",
    "temp = df2.loc[:, ['RecordID']]\n",
    "df2 = pd.DataFrame(scaler.fit_transform(df2[cols_to_scale]), columns=scaler.get_feature_names_out())\n",
    "df2 = pd.concat([temp, df2], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "df1 = scaler.fit_transform(df1)\n",
    "df2 = scaler.transform(df2)\n",
    "\n",
    "pca = PCA(0.95)  # Retain 95% of the variance\n",
    "df1 = pca.fit_transform(df1)\n",
    "df2 = pca.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSplitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.loc[:, df1.columns != 'hospital_death']\n",
    "y = df1['hospital_death']\n",
    "\n",
    "#split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradient Boosting Classifier with tuned hyperparameters\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=200,       # The number of boosting stages to be used\n",
    "    learning_rate=0.05,      # Shrinkage parameter to prevent overfitting\n",
    "    max_depth=6,            # Maximum depth of individual trees\n",
    "    min_samples_split=10,    # Minimum samples required to split a node\n",
    "    min_samples_leaf=5,     # Minimum samples required at each leaf node\n",
    "    subsample=0.8,          # Fraction of samples used for fitting the trees\n",
    "    random_state=42         # Random seed for reproducibility\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(iterations=1700, \n",
    "                         depth=6, \n",
    "                         learning_rate=0.01, \n",
    "                         loss_function='Logloss', \n",
    "                         eval_metric='AUC',\n",
    "                         random_seed=42,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree classifier as the base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=7, random_state=42)\n",
    "\n",
    "#base_estimator = GaussianNB() not good result\n",
    "\n",
    "#base_estimator = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', n_estimators=330, max_depth=6, learning_rate= 0.03349791504065030, subsample=0.923491158880027, gamma=0.09694961288685062, reg_lambda=0.02716045699471643, min_child_weight=4.166361834440882, colsample_bytree=0.672977599702712, colsample_bylevel= 0.6497642793976, scale_pos_weight= 1.10373899695754, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "clf = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                         learning_rate= 0.01,\n",
    "                         n_estimators=150,\n",
    "                         random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassfier (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base classifier (Decision Tree in this example)\n",
    "#base_classifier = DecisionTreeClassifier(max_depth=9, min_samples_leaf=500, min_samples_split=7,random_state=42)\n",
    "\n",
    "#ase_classifier = GaussianNB() not good !!\n",
    "\n",
    "base_classifier = xgb.XGBClassifier(\n",
    "    learning_rate= 0.0334925,\n",
    "    max_depth=6,\n",
    "    n_estimators=376,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    gamma=0.097, \n",
    "    reg_lambda=0.0271605,\n",
    "    min_child_weight=4.166362,\n",
    "    colsample_bytree=0.673,\n",
    "    colsample_bylevel= 0.65,\n",
    "    scale_pos_weight= 1.103739,\n",
    "    subsample=0.7967162407706075\n",
    ")\n",
    "\n",
    "# Create a BaggingClassifier\n",
    "clf = BaggingClassifier(base_classifier, n_estimators=350, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =xgb.XGBClassifier(\n",
    "    learning_rate= 0.0334925,\n",
    "    max_depth=6,\n",
    "    n_estimators=376,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    gamma=0.097, \n",
    "    reg_lambda=0.0271605,\n",
    "    min_child_weight=4.166362,\n",
    "    colsample_bytree=0.673,\n",
    "    colsample_bylevel= 0.65,\n",
    "    scale_pos_weight= 1.103739,\n",
    "    subsample=0.7967162407706075\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalise Lightgbm as clf\n",
    "clf = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    num_leaves=175,\n",
    "    min_child_samples=25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply randomforest classifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremly RandomForestTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ExtraTreesClassifier with appropriate hyperparameters\n",
    "clf = ExtraTreesClassifier(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    max_depth=10,      # Maximum depth of each tree (adjust as needed)\n",
    "    min_samples_split=9,  # Minimum number of samples required to split a node\n",
    "    min_samples_leaf=50,   # Minimum number of samples required at each leaf node\n",
    "    #max_features='sqrt',  # Number of features to consider when looking for the best split\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual classifiers\n",
    "# Replace these with your choice of classifiers\n",
    "classifier1 = CatBoostClassifier(iterations=1700, depth=6, learning_rate=0.01, loss_function='Logloss', eval_metric='AUC', random_seed=42)\n",
    "classifier2 = lgb.LGBMClassifier(objective='binary', learning_rate=0.1, n_estimators=800, max_depth=7, num_leaves=175, min_child_samples=25, random_state=42)\n",
    "classifier3 = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', n_estimators=330, max_depth=6, learning_rate= 0.03349791504065030, subsample=0.923491158880027, gamma=0.09694961288685062, reg_lambda=0.02716045699471643, min_child_weight=4.166361834440882, colsample_bytree=0.672977599702712, colsample_bylevel= 0.6497642793976, scale_pos_weight= 1.10373899695754, random_state=42)\n",
    "\n",
    "# Create a VotingClassifier\n",
    "clf = VotingClassifier(\n",
    "    estimators=[('clf1', classifier1), ('clf2', classifier2), ('clf3', classifier3)],\n",
    "    voting='soft'  # 'soft' for probability voting, 'hard' for majority voting\n",
    ")\n",
    "\n",
    "#clf = BaggingClassifier(clf1, n_estimators=100, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost classifier\n",
    "meta_model = xgb.XGBClassifier(\n",
    "   learning_rate= 0.015,\n",
    "        max_depth=2,\n",
    "        n_estimators=395,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        gamma=0.09, \n",
    "        reg_lambda=0.02,\n",
    "        min_child_weight=4,\n",
    "        colsample_bytree=0.6,\n",
    "        colsample_bylevel= 0.62,\n",
    "        scale_pos_weight= 1.1,\n",
    "        subsample=0.8\n",
    ")\n",
    "\n",
    "# Create a list of estimators including CatBoost and LightGBM\n",
    "estimators = [\n",
    "    ('cat', CatBoostClassifier(\n",
    "   learning_rate=0.050,\n",
    "    n_estimators=476,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=8.0841,\n",
    "    subsample=0.791,\n",
    "    objective='Logloss',\n",
    "    random_state=42,\n",
    ")),\n",
    "    \n",
    "    \n",
    "    \n",
    "    ('lgb', lgb.LGBMClassifier(\n",
    "    learning_rate=0.071,\n",
    "    n_estimators=395,\n",
    "    max_depth=10,\n",
    "    lambda_l2=9.957,\n",
    "    subsample=0.79,\n",
    "    num_leaves=11,\n",
    "    objective='binary',\n",
    "    random_state=42)),\n",
    "     \n",
    "     \n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "   learning_rate= 0.0335,\n",
    "        max_depth=6,\n",
    "        n_estimators=385,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        gamma=0.0969, \n",
    "        reg_lambda=0.0271,\n",
    "        min_child_weight=4.167,\n",
    "        colsample_bytree=0.673,\n",
    "        colsample_bylevel= 0.65,\n",
    "        scale_pos_weight= 1.10,\n",
    "        subsample=0.798\n",
    "))  # Use the XGBoost model we defined earlier\n",
    "]\n",
    "\n",
    "# Create a stacking classifier with XGBoost as the final estimator\n",
    "clf1 = StackingClassifier(estimators=estimators, final_estimator=meta_model)\n",
    "\n",
    "clf = BaggingClassifier(clf1, n_estimators=100, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAT BOOST\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "'''param_grid = {\n",
    "    'iterations': [500,100,1500],      # Number of iterations\n",
    "    'depth': [4,7,9],                # Tree depth\n",
    "    'learning_rate': [0.01,0.001],  # Learning rate\n",
    "    'loss_function': ['Logloss'],       # Loss function\n",
    "    'eval_metric': ['AUC'],             # Evaluation metric\n",
    "}'''\n",
    "#LIGHTGBM\n",
    "param_grid = {\n",
    "    'n_estimators': [200,300,500],    # Number of boosting rounds\n",
    "    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate\n",
    "    'max_depth': [5, 7, 9],            # Maximum depth of trees\n",
    "    'num_leaves': [31, 63, 127],       # Maximum number of leaves in one tree\n",
    "    'min_child_samples': [10, 20, 30],  # Minimum number of data points in leaves\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=3, scoring='roc_auc', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Perform GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and their corresponding AUC score\n",
    "best_params = grid_search.best_params_\n",
    "best_auc = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best AUC Score:\", best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def objective(trial):\n",
    "    # Load and preprocess your training data (X, y)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define XGBoost parameters to search\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 50),\n",
    "        'n_estimators': trial.suggest_loguniform('n_estimators', 100,5000),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "    }\n",
    "\n",
    "    # Create and train the XGBoost model\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    model = xgb.train(params, dtrain, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_prob = model.predict(dval)\n",
    "\n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "    return roc_auc\n",
    "'''\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_float('min_samples_split', 0.01, 1.0),\n",
    "        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.01, 0.5),\n",
    "        'max_features': trial.suggest_int('max_features', 10,100),\n",
    "        #'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "    \n",
    "    # Create the ExtraTreesClassifier with the suggested hyperparameters\n",
    "    clf = ExtraTreesClassifier(**params, random_state=42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = clf.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
    "    \n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return roc_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)  # You can adjust the number of trials\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "best_auc = study.best_value\n",
    "\n",
    "print(f'Best ROC AUC: {best_auc:.4f}')\n",
    "print(f'Best Parameters: {best_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = ExtraTreesClassifier(**best_params, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, you can evaluate the model on a validation or test set.\n",
    "#y_pred = best_clf.predict(df2)\n",
    "#accuracy = accuracy_score(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "pred = clf.predict_proba(df2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(X_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "pred = clf.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = md_pred[:,1]\n",
    "#print mdpred up to 6 decimal places\n",
    "print(pred.round(6))\n",
    "#calculate accuracy\n",
    "score = accuracy_score(X_test, y_pred)\n",
    "print('Accuracy: %.3f' % score)\n",
    "\n",
    "print(classification_report(X_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve and AUC for the ExtraTreesClassifier\n",
    "pred = clf.predict_proba(df2)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc_score = roc_auc_score(y_test, pred)\n",
    "print(f'ROC AUC Score: {roc_auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV FILE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the results with RecordID and predicted probability of death\n",
    "results_df = pd.DataFrame({'row ID': df2['row ID'] , 'price_doc': pred})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('submission2_25253.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
