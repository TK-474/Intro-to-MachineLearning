{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_regression, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import legacy\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"/Users/talalkhan/Documents/Data Sets/Second Challange/train.csv\")\n",
    "test_data = pd.read_csv(r\"/Users/talalkhan/Documents/Data Sets/Second Challange/test.csv\")\n",
    "row_ids = test_data['row ID']\n",
    "test_data = test_data.drop('row ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop sub_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'sub_area' column\n",
    "train_data = train_data.drop('sub_area', axis=1)\n",
    "test_data = test_data.drop('sub_area', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical & Categorical Columns distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "numerical_columns = train_data.select_dtypes(exclude=['object']).columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "train_data_encoded = pd.DataFrame(encoder.fit_transform(train_data[categorical_columns]))\n",
    "train_data_encoded.columns = train_data_encoded.columns.astype(str)\n",
    "train_data = pd.concat([train_data, train_data_encoded], axis=1)\n",
    "train_data = train_data.drop(categorical_columns, axis=1)\n",
    "\n",
    "# Apply the same transformation to the test set\n",
    "test_data_encoded = pd.DataFrame(encoder.transform(test_data[categorical_columns]))\n",
    "test_data_encoded.columns = test_data_encoded.columns.astype(str)\n",
    "test_data = pd.concat([test_data, test_data_encoded], axis=1)\n",
    "test_data = test_data.drop(categorical_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Label Encoding to the categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    train_data[column] = label_encoder.fit_transform(train_data[column])\n",
    "    test_data[column] = label_encoder.transform(test_data[column])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = train_data.drop(columns=['price_doc'])\n",
    "y = train_data['price_doc']\n",
    "X_test = test_data\n",
    "\n",
    "# Convert column names to strings\n",
    "X.columns = X.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling / Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply MinMaxScale\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance based Filter\n",
    "variance_selector = VarianceThreshold(threshold=0.005)\n",
    "X = variance_selector.fit_transform(X)\n",
    "X_test = variance_selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=0.8)\n",
    "X = pca.fit_transform(X)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant column to X and X_test\n",
    "X = sm.add_constant(X)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the ordinary least squares (OLS) model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Get the p-values for each feature\n",
    "p_values = results.pvalues\n",
    "\n",
    "# Select features with p-value less than 0.05 (or any desired threshold)\n",
    "selected_features = p_values[p_values < 0.05].index\n",
    "\n",
    "# Filter X and X_test based on selected features\n",
    "X = X[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Feature Selection\n",
    "model_lasso = Lasso(alpha=0.1)\n",
    "sfs = SequentialFeatureSelector(model_lasso, n_features_to_select=10, direction='forward', scoring='neg_mean_squared_error', cv=5)\n",
    "X_forward = sfs.fit_transform(X, y)\n",
    "X_test_forward = sfs.transform(X_test)\n",
    "\n",
    "# Select final features after forward selection\n",
    "selected_features_forward = [f'PC_{i+1}' for i in sfs.get_support(indices=True)]\n",
    "X = pd.DataFrame(X_forward, columns=selected_features_forward)\n",
    "X_test = pd.DataFrame(X_test_forward, columns=selected_features_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Feature Selection\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_forward = selector.fit_transform(X, y)\n",
    "X_test_forward = selector.transform(X_test)\n",
    "\n",
    "# Select final features after forward selection\n",
    "selected_features_forward = [f'PC_{i+1}' for i in selector.get_support(indices=True)]\n",
    "\n",
    "X = pd.DataFrame(X_forward, columns=selected_features_forward)\n",
    "X_test = pd.DataFrame(X_test_forward, columns=selected_features_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna for CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.2),\n",
    "        'random_strength': trial.suggest_int('random_strength', 0, 100),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        #'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "        'od_wait': trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "\n",
    "    # Create and fit the model\n",
    "    model = CatBoostRegressor(**params, verbose=0)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=30)\n",
    "\n",
    "    # Predict and calculate RMSE\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna for XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 10, 100)\n",
    "\n",
    "    }\n",
    "\n",
    "    # Create and train the XGBoost model\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],verbose=True)\n",
    "\n",
    "    # Predict and calculate RMSE\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    \n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna for GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 14),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 14),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    # Create and fit the model\n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and calculate RMSE\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna for LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Defining the hyperparameters to tune\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 10, 70)\n",
    "    }\n",
    "\n",
    "    # Training the model\n",
    "    model_CV = lgb.LGBMRegressor(**params)\n",
    "    model_CV.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "\n",
    "    # Predicting and calculating RMSE\n",
    "    preds = model_CV.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna for RandomForesstRegessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to be tuned\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "\n",
    "    # Create and train the RandomForestRegressor\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and calculate RMSE\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "print('Best parameters:', best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply neural network\n",
    "model = Sequential()\n",
    "#input layer\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "#hidden layers\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(80, activation='sigmoid', kernel_regularizer=l1(0.001)))\n",
    "# #model.add(Dropout(0.2))\n",
    "model.add(Dense(60, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "# # #model.add(Dropout(0.2))\n",
    "model.add(Dense(40, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "#output layer\n",
    "model.add(Dense(1, activation='linear') )\n",
    "#model.add(Dense(1, activation='linear'))\n",
    "\n",
    "opt = legacy.Adam(learning_rate=0.001)\n",
    "#opt = RMSprop(learning_rate=0.005)\n",
    "#opt = legacy.Adagrad(learning_rate=0.01)\n",
    "#opt = legacy.Adamax(learning_rate=0.001)\n",
    "#opt = legacy.SGD(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=opt , metrics='mse')\n",
    "\n",
    "#apply early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.2, max_depth=5, max_features=&#x27;sqrt&#x27;,\n",
       "                          min_samples_leaf=3, min_samples_split=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.2, max_depth=5, max_features=&#x27;sqrt&#x27;,\n",
       "                          min_samples_leaf=3, min_samples_split=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.2, max_depth=5, max_features='sqrt',\n",
       "                          min_samples_leaf=3, min_samples_split=4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.2, min_samples_split=4, min_samples_leaf=3, max_features='sqrt')\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply XGB boost\n",
    "# model = xgb.XGBRegressor(\n",
    "#     objective='reg:squarederror',\n",
    "#     n_estimators=300,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=10,\n",
    "#     subsample=0.6,\n",
    "#     colsample_bytree=0.9,\n",
    "#     early_stopping_rounds = 50\n",
    "# )\n",
    "model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "#model = xgb.XGBRegressor()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply catboost regressor\n",
    "model = CatBoostRegressor(\n",
    "    iterations=20_00,\n",
    "    learning_rate=0.03,\n",
    "    depth=7,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='RMSE',\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "\n",
    "#model = CatBoostRegressor(**best_params, verbose=0)\n",
    "\n",
    "#model = CatBoostRegressor(depth=)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    verbose=True,  # Output the training progress every 100 iterations\n",
    "    use_best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply lightgbm regressor\n",
    "'''model = lgb.LGBMRegressor(\n",
    "    num_leaves=31, \n",
    "    max_depth=-5, \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=200, \n",
    "    min_data_in_leaf=20, \n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8, \n",
    "    bagging_freq=5, \n",
    "    lambda_l1=0.3, \n",
    "    lambda_l2=0.3\n",
    ")'''\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(ccp_alpha=0, max_features=&#x27;sqrt&#x27;,\n",
       "                      min_impurity_decrease=0.01, min_samples_leaf=3,\n",
       "                      min_samples_split=4, n_estimators=1500, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(ccp_alpha=0, max_features=&#x27;sqrt&#x27;,\n",
       "                      min_impurity_decrease=0.01, min_samples_leaf=3,\n",
       "                      min_samples_split=4, n_estimators=1500, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(ccp_alpha=0, max_features='sqrt',\n",
       "                      min_impurity_decrease=0.01, min_samples_leaf=3,\n",
       "                      min_samples_split=4, n_estimators=1500, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model (Random Forest Regressor in this example)\n",
    "model = RandomForestRegressor(n_estimators=1500, \n",
    "                              #max_depth=10, \n",
    "                              min_samples_split=4, \n",
    "                              min_samples_leaf=3, \n",
    "                              max_leaf_nodes=None,\n",
    "                              max_features='sqrt',\n",
    "                              min_impurity_decrease=0.01,\n",
    "                              ccp_alpha=0,\n",
    "                              oob_score=True, \n",
    "                              bootstrap=True, \n",
    "                              random_state=42,\n",
    "                              n_jobs=-1\n",
    "                              )\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fit for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit for nn\n",
    "model.fit(X_train, y_train, epochs=100 , batch_size=512, validation_data=(X_train,y_train), callbacks=[early_stopping] , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 12931559.271809414\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "\n",
    "#lowest = 12570594.985033065\n",
    "#RFlowest = 12552307.318903735 | 1300 iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten Values if NN used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = test_preds.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test row IDs with their corresponding predictions into a DataFrame\n",
    "output = pd.DataFrame({'row ID': row_ids, 'price_doc': test_preds})\n",
    "\n",
    "# Output the DataFrame to a CSV file\n",
    "output.to_csv('submission143_25253.csv', index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
