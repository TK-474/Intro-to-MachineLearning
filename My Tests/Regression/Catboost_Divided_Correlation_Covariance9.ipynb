{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area_m': -23858099335215.94, 'sub_area': -269308211.6375193, 'metro_min_walk': -108810817.01734817, 'trc_sqm_500': 10731289550.820986, 'work_all': 12986872168.704714, '16_29_all': 13978403395.378593, 'raion_popul': 22765516911.780304, 'male_f': 32947352725.91563, 'female_f': 37977829639.2107, 'office_sqm_500': 49446917519.27475, 'trc_sqm_1000': 66641729039.167076, 'full_all': 70925587054.43039, 'trc_sqm_1500': 149301900627.71182, 'office_sqm_1000': 194917540476.89062, 'trc_sqm_2000': 279898311008.8841, 'office_sqm_1500': 475798732601.1094, 'trc_sqm_3000': 677735345779.0322, 'office_sqm_2000': 852103962369.7913, 'trc_sqm_5000': 1650558510091.0176, 'office_sqm_3000': 1875214633319.1877, 'office_sqm_5000': 4691740388588.324}\n",
      "{'full_all': -113261569321.28836, 'office_sqm_1500': -102137605229.06575, 'trc_sqm_3000': -96676523866.28038, 'trc_sqm_500': -77277241403.57695, 'office_sqm_500': -37338695694.68065, '16_29_male': -21706021677.57643, 'female_f': -19668934306.258545, 'office_sqm_5000': -18483589888.364952, 'trc_sqm_1500': -13850744555.308224, 'trc_sqm_2000': 112436544931.88568, 'trc_sqm_5000': 150544250529.10965, 'office_sqm_3000': 285503985062.2618, 'area_m': 4470705331037.172}\n",
      "[('full_sq', 0.4098492007057082), ('sport_count_3000', 0.3550296705474758), ('sport_count_5000', 0.3490379403370083), ('sport_count_2000', 0.342356639374105), ('office_sqm_5000', 0.33669895136032296), ('trc_count_5000', 0.33121769410123364), ('cafe_count_5000_price_1500', 0.3304423450239086), ('cafe_count_5000_price_1000', 0.3301606404517439), ('cafe_count_5000', 0.32594172796521986), ('cafe_count_5000_na_price', 0.32591742478117025), ('cafe_count_5000_price_2500', 0.32325302234288283), ('cafe_count_5000_price_500', 0.3169871485398577), ('sport_count_1500', 0.31393076380906587), ('church_count_5000', 0.3110426705088716), ('cafe_count_5000_price_4000', 0.31068954559129547), ('office_count_5000', 0.31017966279589715), ('cafe_count_5000_price_high', 0.3085554229709476), ('office_sqm_3000', 0.30604538551809934), ('metro_min_avto', -0.2043344765593499), ('office_km', -0.20679063795217487), ('detention_facility_km', -0.20979298365887375), ('mosque_km', -0.21272390520013512), ('stadium_km', -0.21673325164056817), ('nuclear_reactor_km', -0.22655580519346397), ('ttk_km', -0.25446424469902956), ('zd_vokzaly_avto_km', -0.2597503581406552), ('bulvar_ring_km', -0.27258625721908797), ('sadovoe_km', -0.27350172476025164), ('kremlin_km', -0.27576166518952067)]\n",
      "[('trc_count_1500', 0.021679072979576573), ('sport_count_2000', 0.019538946011091414), ('sadovoe_km', 0.01675180266526467), ('university_top_20_raion', -0.012325641584851573), ('detention_facility_raion', -0.012964508051993147), ('16_29_male', -0.013578194192907825), ('build_count_foam', -0.013643276529975186), ('cafe_count_1500_na_price', -0.016060215270219967)]\n",
      "Final list of selected features:\n",
      "16_29_male\n",
      "area_m\n",
      "build_count_foam\n",
      "cafe_count_1500_na_price\n",
      "detention_facility_raion\n",
      "female_f\n",
      "full_all\n",
      "full_sq\n",
      "office_sqm_1500\n",
      "office_sqm_3000\n",
      "office_sqm_500\n",
      "office_sqm_5000\n",
      "sadovoe_km\n",
      "sport_count_2000\n",
      "trc_count_1500\n",
      "trc_sqm_1500\n",
      "trc_sqm_2000\n",
      "trc_sqm_3000\n",
      "trc_sqm_500\n",
      "trc_sqm_5000\n",
      "university_top_20_raion\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(r\"/Users/talalkhan/Documents/Data Sets/Second Challange/train.csv\")\n",
    "test_data = pd.read_csv(r\"/Users/talalkhan/Documents/Data Sets/Second Challange/test.csv\")\n",
    "train_data = train_data.drop_duplicates()\n",
    "\n",
    "# Encode categorical features with LabelEncoder (if needed)\n",
    "label_encoders = {}\n",
    "for column in train_data.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    train_data[column] = label_encoders[column].fit_transform(train_data[column])\n",
    "    test_data[column] = label_encoders[column].transform(test_data[column])\n",
    "\n",
    "# Divide into whole and non-whole\n",
    "whole_number_rows = train_data[train_data['children_school'] % 1 == 0]\n",
    "non_whole_number_rows = train_data[train_data['children_school'] % 1 != 0]\n",
    "\n",
    "# Function to calculate covariance with price_doc for each column in a given dataset\n",
    "def calculate_covariance_with_price(data):\n",
    "    covariances = {}\n",
    "    for column in data.columns:\n",
    "        if column != 'price_doc':\n",
    "            covariance = data[[column, 'price_doc']].cov().iloc[0, 1]\n",
    "            covariances[column] = covariance\n",
    "    \n",
    "    # Sort the covariances dictionary by values in ascending order\n",
    "    sorted_covariances = dict(sorted(covariances.items(), key=lambda item: item[1]))\n",
    "    return sorted_covariances\n",
    "\n",
    "# Calculate covariance for whole_number_rows\n",
    "covariance_whole = calculate_covariance_with_price(whole_number_rows)\n",
    "\n",
    "# Calculate covariance for non_whole_number_rows\n",
    "covariance_non_whole = calculate_covariance_with_price(non_whole_number_rows)\n",
    "\n",
    "\n",
    "# Filter features with covariance < -10000000 or > 1000000000\n",
    "selected_covariance_whole = {key: value for key, value in covariance_whole.items() if value < -100000000  or value > 10000000000 }\n",
    "print (selected_covariance_whole)\n",
    "\n",
    "selected_covariance_non_whole = {key: value for key, value in covariance_non_whole.items() if value < -10000000000  or value > 100000000000 }\n",
    "print (selected_covariance_non_whole)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate and sort correlations with price_doc for each column in a given dataset\n",
    "def calculate_and_sort_correlation_with_price(data):\n",
    "    correlations = {column: data[[column, 'price_doc']].corr().iloc[0, 1] for column in data.columns if column != 'price_doc'}\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_correlations\n",
    "\n",
    "# Calculate and sort correlation for whole_number_rows\n",
    "sorted_correlation_whole = calculate_and_sort_correlation_with_price(whole_number_rows)\n",
    "\n",
    "# Calculate and sort correlation for non_whole_number_rows\n",
    "sorted_correlation_non_whole = calculate_and_sort_correlation_with_price(non_whole_number_rows)\n",
    "\n",
    "# Filter features with correlation > 0.1 or < -0.1\n",
    "selected_correlation_whole = [item for item in sorted_correlation_whole if item[1] > 0.3 or item[1] < -0.2]\n",
    "print(selected_correlation_whole)\n",
    "\n",
    "selected_correlation_non_whole = [item for item in sorted_correlation_non_whole if item[1] > 0.015 or item[1] < -0.012]\n",
    "print(selected_correlation_non_whole)\n",
    "\n",
    "selected_features_correlation_whole = set([item[0] for item in selected_correlation_whole])\n",
    "selected_features_covariance_whole = set(selected_covariance_whole.keys())\n",
    "\n",
    "# Provided list of features\n",
    "provided_features = [\n",
    "    \"full_sq\", \"floor\", \"cafe_count_5000_price_high\", \"kindergarten_km\", \"life_sq\", \"public_transport_station_km\",\n",
    "    \"public_transport_station_min_walk\", \"green_zone_km\", \"school_km\", \"water_km\", \"green_part_500\", \"fitness_km\",\n",
    "    \"preschool_km\", \"sport_count_500\", \"railroad_km\", \"additional_education_km\"\n",
    "]\n",
    "\n",
    "# Combine all selected and provided features, removing duplicates\n",
    "all_selected_features = selected_features_correlation_whole.union(selected_features_covariance_whole).union(provided_features)\n",
    "\n",
    "# Convert the set to a list and sort it for consistency\n",
    "feature_list_whole = sorted(list(all_selected_features))\n",
    "\n",
    "# Features from selected_correlation_whole and selected_covariance_whole\n",
    "selected_features_correlation_non_whole = set([item[0] for item in selected_correlation_non_whole])\n",
    "selected_features_covariance_non_whole = set(selected_covariance_non_whole.keys())\n",
    "\n",
    "# Provided list of features\n",
    "provided_features1 = [\n",
    "\"full_sq\"\n",
    "]\n",
    "\n",
    "# Combine all selected and provided features, removing duplicates\n",
    "all_selected_features_non_whole = selected_features_correlation_non_whole.union(selected_features_covariance_non_whole).union(provided_features1)\n",
    "\n",
    "# Convert the set to a list and sort it for consistency\n",
    "feature_list_non_whole = sorted(list(all_selected_features_non_whole))\n",
    "\n",
    "# Print the final list of features\n",
    "print(\"Final list of selected features:\")\n",
    "for feature in feature_list_non_whole:\n",
    "    print(feature)\n",
    "    \n",
    "print(len(feature_list_non_whole))\n",
    "\n",
    "\n",
    "# Prepare train and test data with selected features\n",
    "X_whole = whole_number_rows[feature_list_whole]\n",
    "X_non_whole = non_whole_number_rows[feature_list_non_whole]\n",
    "y_whole = whole_number_rows['price_doc']\n",
    "y_non_whole = non_whole_number_rows['price_doc']\n",
    "\n",
    "# Train-test split\n",
    "X_train_whole, X_val_whole, y_train_whole, y_val_whole = train_test_split(X_whole, y_whole, test_size=0.1, random_state=42)\n",
    "X_train_non_whole, X_val_non_whole, y_train_non_whole, y_val_non_whole = train_test_split(X_non_whole, y_non_whole, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(ccp_alpha=0, max_features=&#x27;sqrt&#x27;,\n",
       "                      min_impurity_decrease=0.01, min_samples_leaf=3,\n",
       "                      min_samples_split=4, n_estimators=600, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(ccp_alpha=0, max_features=&#x27;sqrt&#x27;,\n",
       "                      min_impurity_decrease=0.01, min_samples_leaf=3,\n",
       "                      min_samples_split=4, n_estimators=600, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(ccp_alpha=0, max_features='sqrt',\n",
       "                      min_impurity_decrease=0.01, min_samples_leaf=3,\n",
       "                      min_samples_split=4, n_estimators=600, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# CatBoost parameters\n",
    "cat_params = {\n",
    "    'iterations': 100000,\n",
    "    'depth': 12,\n",
    "    'learning_rate': 0.05,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'random_seed': 42,\n",
    "    'l2_leaf_reg': 0.05,\n",
    "    'early_stopping_rounds': 40\n",
    "}\n",
    "\n",
    "# Create and train CatBoost models\n",
    "#best_model_whole = CatBoostRegressor(**cat_params)\n",
    "best_model_whole = RandomForestRegressor(n_estimators=600, \n",
    "                              #max_depth=10, \n",
    "                              min_samples_split=4, \n",
    "                              min_samples_leaf=3, \n",
    "                              max_leaf_nodes=None,\n",
    "                              max_features='sqrt',\n",
    "                              min_impurity_decrease=0.01,\n",
    "                              ccp_alpha=0,\n",
    "                              oob_score=True, \n",
    "                              bootstrap=True, \n",
    "                              random_state=42,\n",
    "                              n_jobs=-1\n",
    "                              )\n",
    "\n",
    "best_model_whole.fit(X_train_whole, y_train_whole)#, eval_set=(X_val_whole, y_val_whole), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 400 out of 400 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the hyperparameters for the Random Forest Regressor\n",
    "rf_params = {\n",
    "    'n_estimators': 400,  # Number of trees in the forest\n",
    "    'max_depth': 3,      # Maximum depth of each tree\n",
    "    'random_state': 42,   # Random seed for reproducibility\n",
    "    'n_jobs': -1,         # Use all available CPU cores for parallel processing\n",
    "    'warm_start' : True,\n",
    "    'verbose': 1}\n",
    "\n",
    "# Create the Random Forest Regressor\n",
    "best_model_non_whole = RandomForestRegressor(**rf_params)\n",
    "\n",
    "# Fit the model to your training data\n",
    "best_model_non_whole.fit(X_train_non_whole, y_train_non_whole)\n",
    "\n",
    "# Prepare test data with selected features\n",
    "test_whole_selected = test_data.loc[test_data['children_school'] % 1 == 0, feature_list_whole + ['row ID']]\n",
    "test_non_whole_selected = test_data.loc[test_data['children_school'] % 1 != 0, feature_list_non_whole + ['row ID']]\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions_whole = best_model_whole.predict(test_whole_selected.drop(['row ID'], axis=1))\n",
    "test_predictions_non_whole = best_model_non_whole.predict(test_non_whole_selected.drop(['row ID'], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row ID     price_doc\n",
      "0           1  1.312578e+07\n",
      "1           2  5.770090e+06\n",
      "2           3  5.318175e+06\n",
      "3           4  5.715682e+06\n",
      "4           5  5.316313e+06\n",
      "...       ...           ...\n",
      "77784   77785  5.687485e+07\n",
      "77785   77786  5.446641e+07\n",
      "77786   77787  3.473750e+06\n",
      "77787   77788  3.473750e+06\n",
      "77788   77789  3.473750e+06\n",
      "\n",
      "[77789 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add predictions back to the dataframes\n",
    "test_whole_selected['price_doc'] = test_predictions_whole\n",
    "test_non_whole_selected['price_doc'] = test_predictions_non_whole\n",
    "\n",
    "# Concatenate the results\n",
    "final_predictions = pd.concat([test_whole_selected[['row ID', 'price_doc']], test_non_whole_selected[['row ID', 'price_doc']]])\n",
    "\n",
    "# Sort by 'row ID'\n",
    "final_predictions_sorted = final_predictions.sort_values(by='row ID')\n",
    "\n",
    "# Output the final predictions\n",
    "print(final_predictions_sorted)\n",
    "\n",
    "# Optional: Save to file\n",
    "final_predictions_sorted.to_csv(\"submission142_25253.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
