{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures,MinMaxScaler,RobustScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/talalkhan/Documents/Data Sets/Second Challange/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/talalkhan/Documents/Data Sets/Second Challange/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['sub_area'], axis=1)\n",
    "X_test = test_df.drop(['row ID','sub_area'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.loc[:, train_df.columns != 'price_doc']\n",
    "y_train = train_df[['price_doc']]\n",
    "#X_test = test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop sub_area_ & target variable select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns starting with 'sub_area_'\n",
    "#sub_area_columns = [col for col in train_df.columns if col.startswith('sub_area_')]\n",
    "#train_df.drop(columns=sub_area_columns, inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop('price_doc', axis=1)\n",
    "y_train = train_df['price_doc']\n",
    "X_test = test_df\n",
    "#X_test = test_df.drop(columns=sub_area_columns)\n",
    "X_test = X_test.drop('row ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "#Apply OneHotEncoding to categorical columns\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_train = ohe.fit_transform(X_train[categorical_cols])\n",
    "X_test = ohe.transform(X_test[categorical_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values handle (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=500)\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>product_type</th>\n",
       "      <th>area_m</th>\n",
       "      <th>raion_popul</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>indust_part</th>\n",
       "      <th>children_preschool</th>\n",
       "      <th>preschool_education_centers_raion</th>\n",
       "      <th>...</th>\n",
       "      <th>cafe_count_5000_price_1500</th>\n",
       "      <th>cafe_count_5000_price_2500</th>\n",
       "      <th>cafe_count_5000_price_4000</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "      <th>big_church_count_5000</th>\n",
       "      <th>church_count_5000</th>\n",
       "      <th>mosque_count_5000</th>\n",
       "      <th>leisure_count_5000</th>\n",
       "      <th>sport_count_5000</th>\n",
       "      <th>market_count_5000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6407578.100</td>\n",
       "      <td>155572.0</td>\n",
       "      <td>0.189727</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>9576.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9589336.912</td>\n",
       "      <td>115352.0</td>\n",
       "      <td>0.372602</td>\n",
       "      <td>0.049637</td>\n",
       "      <td>6880.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4808269.831</td>\n",
       "      <td>101708.0</td>\n",
       "      <td>0.112560</td>\n",
       "      <td>0.118537</td>\n",
       "      <td>5879.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8398460.622</td>\n",
       "      <td>108171.0</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.037316</td>\n",
       "      <td>5706.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>552.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506452.020</td>\n",
       "      <td>43795.0</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.486246</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   full_sq  life_sq  floor  product_type       area_m  raion_popul  \\\n",
       "0     43.0     27.0    4.0             0  6407578.100     155572.0   \n",
       "1     34.0     19.0    3.0             0  9589336.912     115352.0   \n",
       "2     43.0     29.0    2.0             0  4808269.831     101708.0   \n",
       "3     77.0     77.0    4.0             0  8398460.622     108171.0   \n",
       "4     67.0     46.0   14.0             0  7506452.020      43795.0   \n",
       "\n",
       "   green_zone_part  indust_part  children_preschool  \\\n",
       "0         0.189727     0.000070              9576.0   \n",
       "1         0.372602     0.049637              6880.0   \n",
       "2         0.112560     0.118537              5879.0   \n",
       "3         0.015234     0.037316              5706.0   \n",
       "4         0.007670     0.486246              2418.0   \n",
       "\n",
       "   preschool_education_centers_raion  ...  cafe_count_5000_price_1500  \\\n",
       "0                                5.0  ...                        40.0   \n",
       "1                                5.0  ...                        36.0   \n",
       "2                                4.0  ...                        25.0   \n",
       "3                                7.0  ...                       552.0   \n",
       "4                                2.0  ...                       155.0   \n",
       "\n",
       "   cafe_count_5000_price_2500  cafe_count_5000_price_4000  \\\n",
       "0                         9.0                         4.0   \n",
       "1                        15.0                         3.0   \n",
       "2                        10.0                         3.0   \n",
       "3                       319.0                       108.0   \n",
       "4                        62.0                        14.0   \n",
       "\n",
       "   cafe_count_5000_price_high  big_church_count_5000  church_count_5000  \\\n",
       "0                         0.0                   13.0               22.0   \n",
       "1                         0.0                   15.0               29.0   \n",
       "2                         0.0                   11.0               27.0   \n",
       "3                        17.0                  135.0              236.0   \n",
       "4                         1.0                   53.0               78.0   \n",
       "\n",
       "   mosque_count_5000  leisure_count_5000  sport_count_5000  market_count_5000  \n",
       "0                1.0                 0.0              52.0                4.0  \n",
       "1                1.0                10.0              66.0               14.0  \n",
       "2                0.0                 4.0              67.0               10.0  \n",
       "3                2.0                91.0             195.0               14.0  \n",
       "4                1.0                20.0             113.0               17.0  \n",
       "\n",
       "[5 rows x 270 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply roohbust scaler\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply variance threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_train = selector.fit_transform(X_train)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#apply selectkbest\n",
    "selector = SelectKBest(f_regression, k=50)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Create a Sequential Feature Selector object\n",
    "sfs = SequentialFeatureSelector(lr, \n",
    "                                direction='forward',\n",
    "                                n_features_to_select=100)\n",
    "\n",
    "# Fit SFS on the PCA-transformed data\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data to the selected features\n",
    "X_train = sfs.transform(X_train)\n",
    "X_test = sfs.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Value filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              price_doc   R-squared:                       0.627\n",
      "Model:                            OLS   Adj. R-squared:                  0.627\n",
      "Method:                 Least Squares   F-statistic:                     1126.\n",
      "Date:                Thu, 30 Nov 2023   Prob (F-statistic):               0.00\n",
      "Time:                        10:44:18   Log-Likelihood:            -3.2352e+06\n",
      "No. Observations:              181507   AIC:                         6.471e+06\n",
      "Df Residuals:                  181235   BIC:                         6.474e+06\n",
      "Df Model:                         271                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -1.575e+06   3.21e+05     -4.909      0.000    -2.2e+06   -9.46e+05\n",
      "x1          5.096e+06   2.73e+05     18.643      0.000    4.56e+06    5.63e+06\n",
      "x2           2.18e+06   2.44e+05      8.926      0.000     1.7e+06    2.66e+06\n",
      "x3          4.779e+06   2.51e+05     19.074      0.000    4.29e+06    5.27e+06\n",
      "x4          7.523e+05   9.01e+04      8.354      0.000    5.76e+05    9.29e+05\n",
      "x5          1.008e+05   1.28e+05      0.786      0.432   -1.51e+05    3.52e+05\n",
      "x6          1.317e+05   2.48e+05      0.531      0.596   -3.54e+05    6.18e+05\n",
      "x7          9.241e+05   2.71e+05      3.406      0.001    3.92e+05    1.46e+06\n",
      "x8          4.787e+05   1.92e+05      2.488      0.013    1.02e+05    8.56e+05\n",
      "x9         -1.364e+04   1.68e+05     -0.081      0.935   -3.43e+05    3.16e+05\n",
      "x10         1.577e+06   2.73e+05      5.783      0.000    1.04e+06    2.11e+06\n",
      "x11         1.855e+06   2.37e+05      7.811      0.000    1.39e+06    2.32e+06\n",
      "x12        -6.904e+05   2.72e+05     -2.540      0.011   -1.22e+06   -1.58e+05\n",
      "x13         4.661e+05   2.25e+05      2.075      0.038    2.59e+04    9.06e+05\n",
      "x14         2.201e+06   1.72e+05     12.796      0.000    1.86e+06    2.54e+06\n",
      "x15         9.802e+05    1.6e+05      6.144      0.000    6.68e+05    1.29e+06\n",
      "x16         9.903e+05   2.14e+05      4.635      0.000    5.72e+05    1.41e+06\n",
      "x17         3.133e+05   2.22e+05      1.413      0.158   -1.21e+05    7.48e+05\n",
      "x18        -8.617e+05   2.04e+05     -4.228      0.000   -1.26e+06   -4.62e+05\n",
      "x19         8.894e+05   1.26e+05      7.048      0.000    6.42e+05    1.14e+06\n",
      "x20        -7711.9512   2.56e+05     -0.030      0.976   -5.09e+05    4.94e+05\n",
      "x21        -5.478e+05   2.24e+05     -2.444      0.015   -9.87e+05   -1.08e+05\n",
      "x22         6.672e+05   2.61e+05      2.559      0.011    1.56e+05    1.18e+06\n",
      "x23         3.649e+04   1.16e+05      0.314      0.753   -1.91e+05    2.64e+05\n",
      "x24          1.32e+06   1.37e+05      9.668      0.000    1.05e+06    1.59e+06\n",
      "x25         9.214e+05   1.44e+05      6.379      0.000    6.38e+05     1.2e+06\n",
      "x26        -2.355e+05   8.03e+04     -2.933      0.003   -3.93e+05   -7.81e+04\n",
      "x27         5.452e+05    1.3e+05      4.200      0.000    2.91e+05       8e+05\n",
      "x28         6.737e+05   1.19e+05      5.645      0.000     4.4e+05    9.08e+05\n",
      "x29         9.972e+05   1.28e+05      7.813      0.000    7.47e+05    1.25e+06\n",
      "x30        -2.064e+05   1.08e+05     -1.919      0.055   -4.17e+05    4377.756\n",
      "x31        -5.263e+04   2.59e+05     -0.203      0.839    -5.6e+05    4.55e+05\n",
      "x32         1.518e+06   2.57e+05      5.904      0.000    1.01e+06    2.02e+06\n",
      "x33         6.591e+05   2.59e+05      2.547      0.011    1.52e+05    1.17e+06\n",
      "x34         2.119e+05   2.74e+05      0.775      0.438   -3.24e+05    7.48e+05\n",
      "x35        -1.764e+05   2.73e+05     -0.645      0.519   -7.12e+05    3.59e+05\n",
      "x36         1.212e+06   2.73e+05      4.445      0.000    6.77e+05    1.75e+06\n",
      "x37         3.323e+05   2.69e+05      1.236      0.216   -1.95e+05    8.59e+05\n",
      "x38         -6.65e+05   2.63e+05     -2.525      0.012   -1.18e+06   -1.49e+05\n",
      "x39         -5.26e+05   2.68e+05     -1.961      0.050   -1.05e+06    -265.249\n",
      "x40         4.794e+05   2.59e+05      1.851      0.064   -2.81e+04    9.87e+05\n",
      "x41         1.047e+06   2.56e+05      4.088      0.000    5.45e+05    1.55e+06\n",
      "x42         1.079e+06   2.59e+05      4.163      0.000    5.71e+05    1.59e+06\n",
      "x43         2.421e+04   2.73e+05      0.089      0.929   -5.11e+05     5.6e+05\n",
      "x44         2.136e+05   2.74e+05      0.780      0.435   -3.23e+05     7.5e+05\n",
      "x45         1.299e+06   2.72e+05      4.767      0.000    7.65e+05    1.83e+06\n",
      "x46        -1.019e+05   2.72e+05     -0.375      0.708   -6.35e+05    4.31e+05\n",
      "x47         3.158e+04   2.71e+05      0.116      0.907      -5e+05    5.63e+05\n",
      "x48        -4.329e+05    2.7e+05     -1.604      0.109   -9.62e+05    9.61e+04\n",
      "x49         5.515e+05   2.75e+05      2.006      0.045    1.27e+04    1.09e+06\n",
      "x50         4.558e+05   2.73e+05      1.669      0.095   -7.93e+04    9.91e+05\n",
      "x51        -4.564e+05   2.72e+05     -1.676      0.094    -9.9e+05    7.75e+04\n",
      "x52         5.538e+05   2.58e+05      2.142      0.032    4.72e+04    1.06e+06\n",
      "x53        -8.767e+05   2.57e+05     -3.411      0.001   -1.38e+06   -3.73e+05\n",
      "x54         9.702e+05   2.61e+05      3.720      0.000    4.59e+05    1.48e+06\n",
      "x55        -2.503e+05   2.74e+05     -0.915      0.360   -7.87e+05    2.86e+05\n",
      "x56        -7.201e+05   2.75e+05     -2.622      0.009   -1.26e+06   -1.82e+05\n",
      "x57        -5.061e+05   2.74e+05     -1.849      0.064   -1.04e+06    3.03e+04\n",
      "x58         -8.97e+05   2.76e+05     -3.254      0.001   -1.44e+06   -3.57e+05\n",
      "x59        -6.244e+05   2.03e+05     -3.074      0.002   -1.02e+06   -2.26e+05\n",
      "x60         6.286e+05   2.58e+05      2.440      0.015    1.24e+05    1.13e+06\n",
      "x61         6.979e+05   2.26e+05      3.095      0.002    2.56e+05    1.14e+06\n",
      "x62        -2.173e+05   2.33e+05     -0.932      0.351   -6.74e+05     2.4e+05\n",
      "x63         2.704e+06   2.32e+05     11.678      0.000    2.25e+06    3.16e+06\n",
      "x64        -1.067e+06   2.43e+05     -4.389      0.000   -1.54e+06   -5.91e+05\n",
      "x65         6.053e+05   2.44e+05      2.482      0.013    1.27e+05    1.08e+06\n",
      "x66          5.04e+05    2.6e+05      1.940      0.052   -5247.775    1.01e+06\n",
      "x67           6.8e+05   2.42e+05      2.808      0.005    2.05e+05    1.15e+06\n",
      "x68        -2.549e+06   2.74e+05     -9.313      0.000   -3.09e+06   -2.01e+06\n",
      "x69        -2.168e+05   2.37e+05     -0.917      0.359    -6.8e+05    2.47e+05\n",
      "x70         3.617e+05   2.61e+05      1.384      0.166   -1.51e+05    8.74e+05\n",
      "x71         1.063e+05   2.47e+05      0.430      0.667   -3.78e+05    5.91e+05\n",
      "x72        -2.115e+06   2.07e+05    -10.210      0.000   -2.52e+06   -1.71e+06\n",
      "x73           1.3e+06   2.66e+05      4.884      0.000    7.78e+05    1.82e+06\n",
      "x74         3.946e+05   1.55e+05      2.553      0.011    9.17e+04    6.98e+05\n",
      "x75         2.381e+05   2.65e+05      0.900      0.368    -2.8e+05    7.57e+05\n",
      "x76         9.008e+05    2.7e+05      3.333      0.001    3.71e+05    1.43e+06\n",
      "x77        -9.954e+05    2.7e+05     -3.691      0.000   -1.52e+06   -4.67e+05\n",
      "x78        -2.026e+05   2.71e+05     -0.747      0.455   -7.34e+05    3.29e+05\n",
      "x79         1.337e+06   2.56e+05      5.217      0.000    8.35e+05    1.84e+06\n",
      "x80        -6.169e+05   2.74e+05     -2.253      0.024   -1.15e+06   -8.02e+04\n",
      "x81         9.104e+05   2.69e+05      3.388      0.001    3.84e+05    1.44e+06\n",
      "x82         2.287e+06   2.15e+05     10.616      0.000    1.86e+06    2.71e+06\n",
      "x83          3.82e+06   2.63e+05     14.526      0.000     3.3e+06    4.34e+06\n",
      "x84        -1.031e+06   2.36e+05     -4.375      0.000   -1.49e+06   -5.69e+05\n",
      "x85         1.212e+06   2.38e+05      5.097      0.000    7.46e+05    1.68e+06\n",
      "x86         3.039e+05    2.6e+05      1.171      0.242   -2.05e+05    8.13e+05\n",
      "x87         2.856e+05   2.53e+05      1.130      0.258    -2.1e+05    7.81e+05\n",
      "x88        -4.872e+05   2.51e+05     -1.939      0.052    -9.8e+05    5203.412\n",
      "x89        -4.839e+05   1.88e+05     -2.572      0.010   -8.53e+05   -1.15e+05\n",
      "x90         4.963e+05   2.52e+05      1.971      0.049    2708.649     9.9e+05\n",
      "x91         9.294e+05   2.57e+05      3.618      0.000    4.26e+05    1.43e+06\n",
      "x92        -2.543e+05   1.66e+05     -1.533      0.125   -5.79e+05    7.08e+04\n",
      "x93         6.079e+05   2.58e+05      2.353      0.019    1.02e+05    1.11e+06\n",
      "x94         1.182e+06   2.59e+05      4.571      0.000    6.75e+05    1.69e+06\n",
      "x95         1.468e+06   1.92e+05      7.638      0.000    1.09e+06    1.84e+06\n",
      "x96         8.145e+05    1.1e+05      7.374      0.000    5.98e+05    1.03e+06\n",
      "x97        -2.065e+06   2.68e+05     -7.695      0.000   -2.59e+06   -1.54e+06\n",
      "x98        -1.139e+06   2.71e+05     -4.202      0.000   -1.67e+06   -6.08e+05\n",
      "x99         1.371e+06   2.75e+05      4.996      0.000    8.33e+05    1.91e+06\n",
      "x100        7.819e+04   2.74e+05      0.285      0.776    -4.6e+05    6.16e+05\n",
      "x101        7.544e+05   2.74e+05      2.757      0.006    2.18e+05    1.29e+06\n",
      "x102       -1.021e+05   1.92e+05     -0.533      0.594   -4.78e+05    2.73e+05\n",
      "x103        4.066e+05   1.68e+05      2.422      0.015    7.75e+04    7.36e+05\n",
      "x104        9.942e+05   1.21e+05      8.213      0.000    7.57e+05    1.23e+06\n",
      "x105        8.893e+05   2.24e+05      3.975      0.000    4.51e+05    1.33e+06\n",
      "x106       -3.784e+05   1.37e+05     -2.759      0.006   -6.47e+05    -1.1e+05\n",
      "x107        3.596e+05   2.54e+05      1.413      0.158   -1.39e+05    8.58e+05\n",
      "x108        4.771e+05   1.22e+05      3.926      0.000    2.39e+05    7.15e+05\n",
      "x109       -1.283e+06   2.75e+05     -4.673      0.000   -1.82e+06   -7.45e+05\n",
      "x110         8.77e+05    1.5e+05      5.857      0.000    5.84e+05    1.17e+06\n",
      "x111        4.018e+05   2.62e+05      1.534      0.125   -1.12e+05    9.15e+05\n",
      "x112       -2.676e+05   1.48e+05     -1.810      0.070   -5.57e+05    2.21e+04\n",
      "x113        4.361e+05   2.55e+05      1.713      0.087   -6.29e+04    9.35e+05\n",
      "x114        -1.91e+06   2.63e+05     -7.272      0.000   -2.42e+06    -1.4e+06\n",
      "x115        8.249e+04   2.73e+05      0.302      0.763   -4.53e+05    6.18e+05\n",
      "x116       -1.411e+05   2.68e+05     -0.527      0.598   -6.66e+05    3.84e+05\n",
      "x117         8.44e+05   2.72e+05      3.106      0.002    3.11e+05    1.38e+06\n",
      "x118       -4.479e+05   2.71e+05     -1.655      0.098   -9.78e+05    8.25e+04\n",
      "x119       -7.472e+05   2.41e+05     -3.100      0.002   -1.22e+06   -2.75e+05\n",
      "x120        1.545e+05    2.6e+05      0.593      0.553   -3.56e+05    6.65e+05\n",
      "x121        8.028e+05   2.61e+05      3.072      0.002    2.91e+05    1.31e+06\n",
      "x122       -1.251e+06   2.66e+05     -4.696      0.000   -1.77e+06   -7.29e+05\n",
      "x123       -4.285e+05   2.52e+05     -1.697      0.090   -9.23e+05    6.63e+04\n",
      "x124        3.676e+05   2.69e+05      1.366      0.172    -1.6e+05    8.95e+05\n",
      "x125        1.471e+06   2.75e+05      5.354      0.000    9.33e+05    2.01e+06\n",
      "x126        1.005e+06   2.71e+05      3.715      0.000    4.75e+05    1.54e+06\n",
      "x127        3.058e+05   2.65e+05      1.155      0.248   -2.13e+05    8.25e+05\n",
      "x128        1.485e+05   2.69e+05      0.552      0.581   -3.79e+05    6.76e+05\n",
      "x129        5.721e+05   2.69e+05      2.130      0.033    4.56e+04     1.1e+06\n",
      "x130       -9.632e+05    2.7e+05     -3.561      0.000   -1.49e+06   -4.33e+05\n",
      "x131       -8.198e+05   2.58e+05     -3.183      0.001   -1.32e+06   -3.15e+05\n",
      "x132       -6.337e+05   2.42e+05     -2.617      0.009   -1.11e+06   -1.59e+05\n",
      "x133        1.212e+06   2.64e+05      4.584      0.000    6.94e+05    1.73e+06\n",
      "x134       -1.877e+05   2.73e+05     -0.688      0.492   -7.23e+05    3.47e+05\n",
      "x135        4.057e+05   2.71e+05      1.498      0.134   -1.25e+05    9.37e+05\n",
      "x136        2.399e+06   2.61e+05      9.203      0.000    1.89e+06    2.91e+06\n",
      "x137        5.112e+04   2.58e+05      0.198      0.843   -4.54e+05    5.57e+05\n",
      "x138        7.407e+05   2.65e+05      2.799      0.005    2.22e+05    1.26e+06\n",
      "x139       -6.069e+05   2.66e+05     -2.285      0.022   -1.13e+06   -8.64e+04\n",
      "x140       -7.526e+04    2.7e+05     -0.279      0.781   -6.05e+05    4.54e+05\n",
      "x141        1.934e+06   2.32e+05      8.344      0.000    1.48e+06    2.39e+06\n",
      "x142        5.231e+05   1.18e+05      4.438      0.000    2.92e+05    7.54e+05\n",
      "x143        2.112e+06   2.18e+05      9.691      0.000    1.69e+06    2.54e+06\n",
      "x144        2.048e+06   2.31e+05      8.882      0.000     1.6e+06     2.5e+06\n",
      "x145        7.699e+05   2.67e+05      2.882      0.004    2.46e+05    1.29e+06\n",
      "x146        8.583e+05   2.54e+05      3.382      0.001    3.61e+05    1.36e+06\n",
      "x147       -1.037e+05   2.24e+05     -0.463      0.643   -5.42e+05    3.35e+05\n",
      "x148        1.855e+06   2.68e+05      6.912      0.000    1.33e+06    2.38e+06\n",
      "x149        9.976e+05   2.71e+05      3.678      0.000    4.66e+05    1.53e+06\n",
      "x150        4.265e+05   2.64e+05      1.615      0.106   -9.11e+04    9.44e+05\n",
      "x151         4.06e+05   2.69e+05      1.511      0.131   -1.21e+05    9.33e+05\n",
      "x152        2.095e+06   2.69e+05      7.787      0.000    1.57e+06    2.62e+06\n",
      "x153        2.244e+06    2.7e+05      8.324      0.000    1.72e+06    2.77e+06\n",
      "x154       -4.695e+05   2.71e+05     -1.735      0.083      -1e+06    6.08e+04\n",
      "x155        1.608e+06   2.69e+05      5.967      0.000    1.08e+06    2.14e+06\n",
      "x156        2.688e+06   2.61e+05     10.287      0.000    2.18e+06     3.2e+06\n",
      "x157         6.67e+05   2.67e+05      2.499      0.012    1.44e+05    1.19e+06\n",
      "x158        4.336e+05   2.66e+05      1.629      0.103    -8.8e+04    9.55e+05\n",
      "x159        3.069e+06   2.53e+05     12.119      0.000    2.57e+06    3.57e+06\n",
      "x160        4.867e+06   2.62e+05     18.607      0.000    4.35e+06    5.38e+06\n",
      "x161        4.463e+05   2.23e+05      2.003      0.045    9622.814    8.83e+05\n",
      "x162        1.373e+06   2.38e+05      5.780      0.000    9.08e+05    1.84e+06\n",
      "x163        2.779e+06   2.41e+05     11.509      0.000    2.31e+06    3.25e+06\n",
      "x164        1.149e+06    2.3e+05      4.987      0.000    6.98e+05     1.6e+06\n",
      "x165        2.324e+06    2.7e+05      8.598      0.000    1.79e+06    2.85e+06\n",
      "x166        2.485e+06   2.68e+05      9.272      0.000    1.96e+06    3.01e+06\n",
      "x167       -1.372e+06   2.49e+05     -5.501      0.000   -1.86e+06   -8.83e+05\n",
      "x168        1.431e+06   2.54e+05      5.643      0.000    9.34e+05    1.93e+06\n",
      "x169        6.646e+04   2.77e+05      0.240      0.810   -4.77e+05     6.1e+05\n",
      "x170        8.243e+05   2.64e+05      3.118      0.002    3.06e+05    1.34e+06\n",
      "x171        -5.34e+05   2.75e+05     -1.941      0.052   -1.07e+06    5120.229\n",
      "x172        5.623e+05   2.75e+05      2.048      0.041    2.41e+04     1.1e+06\n",
      "x173         2.19e+06   2.76e+05      7.946      0.000    1.65e+06    2.73e+06\n",
      "x174       -3.981e+05   2.77e+05     -1.439      0.150    -9.4e+05    1.44e+05\n",
      "x175       -7536.8079   2.73e+05     -0.028      0.978   -5.44e+05    5.28e+05\n",
      "x176        3.371e+06   2.66e+05     12.663      0.000    2.85e+06    3.89e+06\n",
      "x177        1.977e+06   2.71e+05      7.299      0.000    1.45e+06    2.51e+06\n",
      "x178       -8.885e+05   2.69e+05     -3.302      0.001   -1.42e+06   -3.61e+05\n",
      "x179        2.248e+06   2.18e+05     10.333      0.000    1.82e+06    2.67e+06\n",
      "x180        3.537e+06   2.73e+05     12.954      0.000       3e+06    4.07e+06\n",
      "x181        2.714e+05   2.43e+05      1.115      0.265   -2.06e+05    7.49e+05\n",
      "x182        2.083e+06   2.35e+05      8.881      0.000    1.62e+06    2.54e+06\n",
      "x183        1.367e+06   2.46e+05      5.548      0.000    8.84e+05    1.85e+06\n",
      "x184       -3.441e+05   2.36e+05     -1.455      0.146   -8.08e+05    1.19e+05\n",
      "x185         8.51e+04   2.76e+05      0.309      0.757   -4.55e+05    6.25e+05\n",
      "x186        8.513e+05   2.55e+05      3.336      0.001    3.51e+05    1.35e+06\n",
      "x187          1.6e+06   2.49e+05      6.427      0.000    1.11e+06    2.09e+06\n",
      "x188        5.171e+04   2.35e+05      0.221      0.825   -4.08e+05    5.11e+05\n",
      "x189       -6.192e+05   2.77e+05     -2.239      0.025   -1.16e+06   -7.72e+04\n",
      "x190       -9.409e+05   2.72e+05     -3.459      0.001   -1.47e+06   -4.08e+05\n",
      "x191       -1.467e+05   2.76e+05     -0.532      0.594   -6.87e+05    3.93e+05\n",
      "x192       -8.981e+04   2.74e+05     -0.327      0.743   -6.28e+05    4.48e+05\n",
      "x193        3.529e+05   2.75e+05      1.282      0.200   -1.87e+05    8.92e+05\n",
      "x194        1.055e+06   2.76e+05      3.819      0.000    5.14e+05     1.6e+06\n",
      "x195        -6.06e+05   2.71e+05     -2.238      0.025   -1.14e+06   -7.54e+04\n",
      "x196        4.955e+05   2.69e+05      1.841      0.066   -3.21e+04    1.02e+06\n",
      "x197        4.157e+05    2.7e+05      1.541      0.123   -1.13e+05    9.45e+05\n",
      "x198        1.215e+06   2.72e+05      4.462      0.000    6.81e+05    1.75e+06\n",
      "x199         1.65e+06   1.95e+05      8.477      0.000    1.27e+06    2.03e+06\n",
      "x200        1.722e+06   2.72e+05      6.333      0.000    1.19e+06    2.25e+06\n",
      "x201        4.413e+05   2.45e+05      1.798      0.072   -3.97e+04    9.22e+05\n",
      "x202        6.676e+05   2.25e+05      2.965      0.003    2.26e+05    1.11e+06\n",
      "x203        -1.04e+06   2.38e+05     -4.376      0.000   -1.51e+06   -5.74e+05\n",
      "x204       -1.799e+04   2.36e+05     -0.076      0.939   -4.81e+05    4.45e+05\n",
      "x205       -1.197e+06   2.75e+05     -4.361      0.000   -1.74e+06   -6.59e+05\n",
      "x206        7.748e+05   2.52e+05      3.077      0.002    2.81e+05    1.27e+06\n",
      "x207       -2.302e+05    2.5e+05     -0.921      0.357    -7.2e+05     2.6e+05\n",
      "x208        1.605e+06   2.44e+05      6.581      0.000    1.13e+06    2.08e+06\n",
      "x209       -5.525e+05   2.76e+05     -1.999      0.046   -1.09e+06   -1.07e+04\n",
      "x210        1.916e+05   2.68e+05      0.716      0.474   -3.33e+05    7.16e+05\n",
      "x211        1.831e+06   2.72e+05      6.726      0.000     1.3e+06    2.36e+06\n",
      "x212        4.881e+05    2.7e+05      1.805      0.071   -4.18e+04    1.02e+06\n",
      "x213       -1.744e+05   2.74e+05     -0.636      0.525   -7.12e+05    3.63e+05\n",
      "x214       -2.559e+05   2.75e+05     -0.929      0.353   -7.96e+05    2.84e+05\n",
      "x215       -1.128e+06   2.74e+05     -4.110      0.000   -1.67e+06    -5.9e+05\n",
      "x216       -1.342e+06   2.74e+05     -4.896      0.000   -1.88e+06   -8.05e+05\n",
      "x217       -3.887e+05   2.76e+05     -1.406      0.160   -9.31e+05    1.53e+05\n",
      "x218        -1.62e+06   2.72e+05     -5.955      0.000   -2.15e+06   -1.09e+06\n",
      "x219        1.347e+06   2.69e+05      5.009      0.000     8.2e+05    1.87e+06\n",
      "x220        7.002e+05   2.73e+05      2.567      0.010    1.65e+05    1.23e+06\n",
      "x221       -4.069e+04   2.71e+05     -0.150      0.881   -5.72e+05    4.91e+05\n",
      "x222       -1.217e+06   1.64e+05     -7.403      0.000   -1.54e+06   -8.94e+05\n",
      "x223       -3.372e+05   2.71e+05     -1.244      0.213   -8.68e+05    1.94e+05\n",
      "x224        2.552e+06   2.54e+05     10.057      0.000    2.05e+06    3.05e+06\n",
      "x225       -3.845e+05    2.2e+05     -1.747      0.081   -8.16e+05    4.68e+04\n",
      "x226       -3.209e+05   2.37e+05     -1.353      0.176   -7.86e+05    1.44e+05\n",
      "x227        8.571e+05    2.3e+05      3.720      0.000    4.05e+05    1.31e+06\n",
      "x228       -7.271e+05   2.76e+05     -2.638      0.008   -1.27e+06   -1.87e+05\n",
      "x229        2.697e+05    2.6e+05      1.038      0.299   -2.39e+05    7.79e+05\n",
      "x230        6.133e+05   2.57e+05      2.391      0.017    1.11e+05    1.12e+06\n",
      "x231        2.877e+05   2.23e+05      1.288      0.198    -1.5e+05    7.25e+05\n",
      "x232        6.855e+05   2.75e+05      2.489      0.013    1.46e+05    1.23e+06\n",
      "x233        2.476e+05   2.68e+05      0.924      0.356   -2.78e+05    7.73e+05\n",
      "x234        5.863e+05   2.71e+05      2.164      0.030    5.52e+04    1.12e+06\n",
      "x235         6.57e+05   2.68e+05      2.449      0.014    1.31e+05    1.18e+06\n",
      "x236       -9.008e+05   2.74e+05     -3.286      0.001   -1.44e+06   -3.64e+05\n",
      "x237       -6.881e+05   2.75e+05     -2.503      0.012   -1.23e+06   -1.49e+05\n",
      "x238       -5.838e+05   2.74e+05     -2.128      0.033   -1.12e+06    -4.6e+04\n",
      "x239       -7.261e+05   2.75e+05     -2.641      0.008   -1.26e+06   -1.87e+05\n",
      "x240       -5.414e+05   2.74e+05     -1.979      0.048   -1.08e+06   -5259.689\n",
      "x241       -6.422e+05    2.7e+05     -2.380      0.017   -1.17e+06   -1.13e+05\n",
      "x242        8.204e+04   2.67e+05      0.308      0.758   -4.41e+05    6.05e+05\n",
      "x243       -6.199e+05   2.71e+05     -2.289      0.022   -1.15e+06   -8.91e+04\n",
      "x244       -1.531e+05   2.72e+05     -0.562      0.574   -6.87e+05    3.81e+05\n",
      "x245       -8.281e+05   1.94e+05     -4.266      0.000   -1.21e+06   -4.48e+05\n",
      "x246       -7.307e+05   2.73e+05     -2.673      0.008   -1.27e+06   -1.95e+05\n",
      "x247        8.242e+05   2.56e+05      3.215      0.001    3.22e+05    1.33e+06\n",
      "x248       -4.718e+05   2.02e+05     -2.333      0.020   -8.68e+05   -7.55e+04\n",
      "x249        8.138e+05   2.38e+05      3.419      0.001    3.47e+05    1.28e+06\n",
      "x250       -1.245e+06   2.12e+05     -5.871      0.000   -1.66e+06    -8.3e+05\n",
      "x251       -4.055e+05    2.7e+05     -1.504      0.133   -9.34e+05    1.23e+05\n",
      "x252        9.238e+05   2.61e+05      3.540      0.000    4.12e+05    1.44e+06\n",
      "x253        5.195e+05    2.6e+05      1.999      0.046    1.02e+04    1.03e+06\n",
      "x254        1.052e+06   2.27e+05      4.634      0.000    6.07e+05     1.5e+06\n",
      "x255       -9.582e+05   2.74e+05     -3.503      0.000   -1.49e+06   -4.22e+05\n",
      "x256        3.182e+05   2.71e+05      1.173      0.241   -2.14e+05     8.5e+05\n",
      "x257         -2.3e+04   2.73e+05     -0.084      0.933   -5.57e+05    5.11e+05\n",
      "x258        9.141e+05   2.71e+05      3.370      0.001    3.83e+05    1.45e+06\n",
      "x259        1.373e+05   2.71e+05      0.506      0.613   -3.94e+05    6.69e+05\n",
      "x260       -3.964e+05   2.71e+05     -1.464      0.143   -9.27e+05    1.34e+05\n",
      "x261       -5.457e+05   2.72e+05     -2.005      0.045   -1.08e+06   -1.22e+04\n",
      "x262       -1.922e+05    2.7e+05     -0.711      0.477   -7.22e+05    3.38e+05\n",
      "x263        -1.08e+05   2.71e+05     -0.398      0.691   -6.39e+05    4.24e+05\n",
      "x264        4.769e+04   2.65e+05      0.180      0.857   -4.71e+05    5.66e+05\n",
      "x265        8.901e+04   2.57e+05      0.347      0.729   -4.14e+05    5.92e+05\n",
      "x266         8.34e+04   2.62e+05      0.318      0.751   -4.31e+05    5.98e+05\n",
      "x267         5.25e+05   2.67e+05      1.967      0.049    1856.390    1.05e+06\n",
      "x268        7.041e+05    1.5e+05      4.698      0.000     4.1e+05    9.98e+05\n",
      "x269        1.533e+05   2.68e+05      0.572      0.567   -3.72e+05    6.79e+05\n",
      "x270        2.205e+05   2.59e+05      0.851      0.395   -2.87e+05    7.28e+05\n",
      "x271        9.136e+05   2.15e+05      4.239      0.000    4.91e+05    1.34e+06\n",
      "==============================================================================\n",
      "Omnibus:                    27294.549   Durbin-Watson:                   1.872\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           391391.484\n",
      "Skew:                           0.231   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.179   Cond. No.                         50.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Add a constant to the features, as statsmodels does not include it by default\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the summary of the model to see the p-values\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/talalkhan/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/Documents/IBA/Semester 5/Intro. To Machine Learning/My Tests/SecondChallangev1.01.ipynb Cell 31\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/talalkhan/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/Documents/IBA/Semester%205/Intro.%20To%20Machine%20Learning/My%20Tests/SecondChallangev1.01.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m p_value_threshold \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/talalkhan/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/Documents/IBA/Semester%205/Intro.%20To%20Machine%20Learning/My%20Tests/SecondChallangev1.01.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Filter features based on the p-value\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/talalkhan/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/Documents/IBA/Semester%205/Intro.%20To%20Machine%20Learning/My%20Tests/SecondChallangev1.01.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m significant_features \u001b[39m=\u001b[39m [feature \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m X_train\u001b[39m.\u001b[39;49mcolumns \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mpvalues[feature] \u001b[39m<\u001b[39m p_value_threshold]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/talalkhan/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/Documents/IBA/Semester%205/Intro.%20To%20Machine%20Learning/My%20Tests/SecondChallangev1.01.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m significant_features2 \u001b[39m=\u001b[39m [feature \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m X_test\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mpvalues[feature] \u001b[39m<\u001b[39m p_value_threshold]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/talalkhan/Library/CloudStorage/OneDrive-InstituteofBusinessAdministration/Documents/IBA/Semester%205/Intro.%20To%20Machine%20Learning/My%20Tests/SecondChallangev1.01.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Select only significant features for training\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Set a threshold for p-values\n",
    "p_value_threshold = 0.05\n",
    "\n",
    "# Filter features based on the p-value\n",
    "significant_features = [feature for feature in X_train.columns if model.pvalues[feature] < p_value_threshold]\n",
    "significant_features2 = [feature for feature in X_test.columns if model.pvalues[feature] < p_value_threshold]\n",
    "\n",
    "# Select only significant features for training\n",
    "X_train = X_train[significant_features]\n",
    "X_test = X_test[significant_features2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply polunomial features\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply ridge model\n",
    "model = Ridge(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_31 (Dense)            (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16897 (66.00 KB)\n",
      "Trainable params: 16897 (66.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "n_features = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=n_features, kernel_regularizer=l1(0.001)))\n",
    "#model.add(Dropout(0.3))  \n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l1(0.001)))\n",
    "#model.add(Dropout(0.3))  \n",
    "model.add(Dense(32, activation='relu',kernel_regularizer=l1(0.001)))\n",
    "#model.add(Dropout(0.3)) \n",
    "#model.add(Dense(25, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer for regression\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['AUC'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1117/1117 [==============================] - 1s 1ms/step - loss: 629483694456832.0000 - auc: 0.0000e+00 - val_loss: 427361795309568.0000 - val_auc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1117/1117 [==============================] - 1s 834us/step - loss: 221420747489280.0000 - auc: 0.0000e+00 - val_loss: 220025386434560.0000 - val_auc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1117/1117 [==============================] - 1s 836us/step - loss: 180695850811392.0000 - auc: 0.0000e+00 - val_loss: 210778690945024.0000 - val_auc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1117/1117 [==============================] - 1s 865us/step - loss: 179095472177152.0000 - auc: 0.0000e+00 - val_loss: 206845826301952.0000 - val_auc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1117/1117 [==============================] - 1s 831us/step - loss: 178468893491200.0000 - auc: 0.0000e+00 - val_loss: 206822908624896.0000 - val_auc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1117/1117 [==============================] - 1s 838us/step - loss: 178063404957696.0000 - auc: 0.0000e+00 - val_loss: 206823042842624.0000 - val_auc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1117/1117 [==============================] - 1s 857us/step - loss: 177760223887360.0000 - auc: 0.0000e+00 - val_loss: 206811768553472.0000 - val_auc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1117/1117 [==============================] - 1s 894us/step - loss: 177467344027648.0000 - auc: 0.0000e+00 - val_loss: 206799789621248.0000 - val_auc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1117/1117 [==============================] - 1s 863us/step - loss: 177264406822912.0000 - auc: 0.0000e+00 - val_loss: 207237842731008.0000 - val_auc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1117/1117 [==============================] - 1s 864us/step - loss: 177077340864512.0000 - auc: 0.0000e+00 - val_loss: 209282297495552.0000 - val_auc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1117/1117 [==============================] - 1s 883us/step - loss: 176913058365440.0000 - auc: 0.0000e+00 - val_loss: 209237032566784.0000 - val_auc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1117/1117 [==============================] - 1s 894us/step - loss: 176784930766848.0000 - auc: 0.0000e+00 - val_loss: 209431832821760.0000 - val_auc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1117/1117 [==============================] - 1s 862us/step - loss: 176675090333696.0000 - auc: 0.0000e+00 - val_loss: 210849272692736.0000 - val_auc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1117/1117 [==============================] - 1s 859us/step - loss: 176589073547264.0000 - auc: 0.0000e+00 - val_loss: 211168996098048.0000 - val_auc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1117/1117 [==============================] - 1s 864us/step - loss: 176451198386176.0000 - auc: 0.0000e+00 - val_loss: 213575788396544.0000 - val_auc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1117/1117 [==============================] - 1s 898us/step - loss: 176430948286464.0000 - auc: 0.0000e+00 - val_loss: 212940452003840.0000 - val_auc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1117/1117 [==============================] - 1s 863us/step - loss: 176342767239168.0000 - auc: 0.0000e+00 - val_loss: 214126668283904.0000 - val_auc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1117/1117 [==============================] - 1s 867us/step - loss: 176286345461760.0000 - auc: 0.0000e+00 - val_loss: 215058525192192.0000 - val_auc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1117/1117 [==============================] - 1s 894us/step - loss: 176243748110336.0000 - auc: 0.0000e+00 - val_loss: 215793618911232.0000 - val_auc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1117/1117 [==============================] - 1s 882us/step - loss: 176150466789376.0000 - auc: 0.0000e+00 - val_loss: 218370012086272.0000 - val_auc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1117/1117 [==============================] - 1s 858us/step - loss: 176138504634368.0000 - auc: 0.0000e+00 - val_loss: 217143211720704.0000 - val_auc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1117/1117 [==============================] - 1s 840us/step - loss: 176103373144064.0000 - auc: 0.0000e+00 - val_loss: 217165693190144.0000 - val_auc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1117/1117 [==============================] - 1s 869us/step - loss: 176054098460672.0000 - auc: 0.0000e+00 - val_loss: 218089715138560.0000 - val_auc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1117/1117 [==============================] - 1s 835us/step - loss: 175976252178432.0000 - auc: 0.0000e+00 - val_loss: 218799861137408.0000 - val_auc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1117/1117 [==============================] - 1s 839us/step - loss: 175980849135616.0000 - auc: 0.0000e+00 - val_loss: 218551323459584.0000 - val_auc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1117/1117 [==============================] - 1s 873us/step - loss: 175923856932864.0000 - auc: 0.0000e+00 - val_loss: 219387449573376.0000 - val_auc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1117/1117 [==============================] - 1s 845us/step - loss: 175883893604352.0000 - auc: 0.0000e+00 - val_loss: 218742936043520.0000 - val_auc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1117/1117 [==============================] - 1s 837us/step - loss: 175851463245824.0000 - auc: 0.0000e+00 - val_loss: 219301902548992.0000 - val_auc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1117/1117 [==============================] - 1s 830us/step - loss: 175815140573184.0000 - auc: 0.0000e+00 - val_loss: 219818892460032.0000 - val_auc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1117/1117 [==============================] - 1s 869us/step - loss: 175782575996928.0000 - auc: 0.0000e+00 - val_loss: 219807114854400.0000 - val_auc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1117/1117 [==============================] - 1s 837us/step - loss: 175763080871936.0000 - auc: 0.0000e+00 - val_loss: 220800812580864.0000 - val_auc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1117/1117 [==============================] - 1s 829us/step - loss: 175758702018560.0000 - auc: 0.0000e+00 - val_loss: 219933531176960.0000 - val_auc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1117/1117 [==============================] - 1s 853us/step - loss: 175693639974912.0000 - auc: 0.0000e+00 - val_loss: 219690899079168.0000 - val_auc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1117/1117 [==============================] - 1s 859us/step - loss: 175653659869184.0000 - auc: 0.0000e+00 - val_loss: 219237293490176.0000 - val_auc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1117/1117 [==============================] - 1s 836us/step - loss: 175638979805184.0000 - auc: 0.0000e+00 - val_loss: 219979500748800.0000 - val_auc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1117/1117 [==============================] - 1s 833us/step - loss: 175610324320256.0000 - auc: 0.0000e+00 - val_loss: 219050093314048.0000 - val_auc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1117/1117 [==============================] - 1s 857us/step - loss: 175577709412352.0000 - auc: 0.0000e+00 - val_loss: 218851249750016.0000 - val_auc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1117/1117 [==============================] - 1s 827us/step - loss: 175556033249280.0000 - auc: 0.0000e+00 - val_loss: 220220035694592.0000 - val_auc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1117/1117 [==============================] - 1s 828us/step - loss: 175541588066304.0000 - auc: 0.0000e+00 - val_loss: 219974836682752.0000 - val_auc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1117/1117 [==============================] - 1s 827us/step - loss: 175515398832128.0000 - auc: 0.0000e+00 - val_loss: 220054880780288.0000 - val_auc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1117/1117 [==============================] - 1s 866us/step - loss: 175491105423360.0000 - auc: 0.0000e+00 - val_loss: 220487279968256.0000 - val_auc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1117/1117 [==============================] - 1s 830us/step - loss: 175465302065152.0000 - auc: 0.0000e+00 - val_loss: 220164217896960.0000 - val_auc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1117/1117 [==============================] - 1s 857us/step - loss: 175444431208448.0000 - auc: 0.0000e+00 - val_loss: 219626457792512.0000 - val_auc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1117/1117 [==============================] - 1s 878us/step - loss: 175417386336256.0000 - auc: 0.0000e+00 - val_loss: 218881230635008.0000 - val_auc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1117/1117 [==============================] - 1s 851us/step - loss: 175378681298944.0000 - auc: 0.0000e+00 - val_loss: 219026336776192.0000 - val_auc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1117/1117 [==============================] - 1s 851us/step - loss: 175385274744832.0000 - auc: 0.0000e+00 - val_loss: 219718145277952.0000 - val_auc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1117/1117 [==============================] - 1s 895us/step - loss: 175360461242368.0000 - auc: 0.0000e+00 - val_loss: 219467644665856.0000 - val_auc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1117/1117 [==============================] - 1s 861us/step - loss: 175308049219584.0000 - auc: 0.0000e+00 - val_loss: 220456124678144.0000 - val_auc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1117/1117 [==============================] - 1s 866us/step - loss: 175285215428608.0000 - auc: 0.0000e+00 - val_loss: 218887270432768.0000 - val_auc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1117/1117 [==============================] - 1s 862us/step - loss: 175298754641920.0000 - auc: 0.0000e+00 - val_loss: 220328852717568.0000 - val_auc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1117/1117 [==============================] - 1s 907us/step - loss: 175255570087936.0000 - auc: 0.0000e+00 - val_loss: 219490579120128.0000 - val_auc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1117/1117 [==============================] - 1s 831us/step - loss: 175228642656256.0000 - auc: 0.0000e+00 - val_loss: 218513205624832.0000 - val_auc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1117/1117 [==============================] - 1s 852us/step - loss: 175200742146048.0000 - auc: 0.0000e+00 - val_loss: 220686794620928.0000 - val_auc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1117/1117 [==============================] - 1s 875us/step - loss: 175192202543104.0000 - auc: 0.0000e+00 - val_loss: 219775506579456.0000 - val_auc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1117/1117 [==============================] - 1s 846us/step - loss: 175146602070016.0000 - auc: 0.0000e+00 - val_loss: 217757878583296.0000 - val_auc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1117/1117 [==============================] - 1s 839us/step - loss: 175152155328512.0000 - auc: 0.0000e+00 - val_loss: 219170922823680.0000 - val_auc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1117/1117 [==============================] - 1s 880us/step - loss: 175104021495808.0000 - auc: 0.0000e+00 - val_loss: 218493895049216.0000 - val_auc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1117/1117 [==============================] - 1s 856us/step - loss: 175110581387264.0000 - auc: 0.0000e+00 - val_loss: 218684467445760.0000 - val_auc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1117/1117 [==============================] - 1s 847us/step - loss: 175089458872320.0000 - auc: 0.0000e+00 - val_loss: 220655723216896.0000 - val_auc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1117/1117 [==============================] - 1s 872us/step - loss: 175070517395456.0000 - auc: 0.0000e+00 - val_loss: 221269182119936.0000 - val_auc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1117/1117 [==============================] - 1s 836us/step - loss: 175082043342848.0000 - auc: 0.0000e+00 - val_loss: 219240229502976.0000 - val_auc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1117/1117 [==============================] - 1s 837us/step - loss: 175043623518208.0000 - auc: 0.0000e+00 - val_loss: 218899064815616.0000 - val_auc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1117/1117 [==============================] - 1s 856us/step - loss: 175026812747776.0000 - auc: 0.0000e+00 - val_loss: 218909147922432.0000 - val_auc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1117/1117 [==============================] - 1s 879us/step - loss: 175014716375040.0000 - auc: 0.0000e+00 - val_loss: 218403398746112.0000 - val_auc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1117/1117 [==============================] - 1s 844us/step - loss: 174993275092992.0000 - auc: 0.0000e+00 - val_loss: 218596185735168.0000 - val_auc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1117/1117 [==============================] - 1s 858us/step - loss: 174957975830528.0000 - auc: 0.0000e+00 - val_loss: 217953752580096.0000 - val_auc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1117/1117 [==============================] - 1s 868us/step - loss: 174922810785792.0000 - auc: 0.0000e+00 - val_loss: 219358223663104.0000 - val_auc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1117/1117 [==============================] - 1s 837us/step - loss: 174941886480384.0000 - auc: 0.0000e+00 - val_loss: 218111676514304.0000 - val_auc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1117/1117 [==============================] - 1s 838us/step - loss: 174914136965120.0000 - auc: 0.0000e+00 - val_loss: 217316168040448.0000 - val_auc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1117/1117 [==============================] - 1s 869us/step - loss: 174872747573248.0000 - auc: 0.0000e+00 - val_loss: 219630215888896.0000 - val_auc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1117/1117 [==============================] - 1s 842us/step - loss: 174898064392192.0000 - auc: 0.0000e+00 - val_loss: 218127816196096.0000 - val_auc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1117/1117 [==============================] - 1s 849us/step - loss: 174847481085952.0000 - auc: 0.0000e+00 - val_loss: 218636367167488.0000 - val_auc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1117/1117 [==============================] - 1s 882us/step - loss: 174841021857792.0000 - auc: 0.0000e+00 - val_loss: 217804267585536.0000 - val_auc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1117/1117 [==============================] - 1s 839us/step - loss: 174846877106176.0000 - auc: 0.0000e+00 - val_loss: 218051127541760.0000 - val_auc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1117/1117 [==============================] - 1s 839us/step - loss: 174767436988416.0000 - auc: 0.0000e+00 - val_loss: 217992776384512.0000 - val_auc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1117/1117 [==============================] - 1s 879us/step - loss: 174797585645568.0000 - auc: 0.0000e+00 - val_loss: 218538287562752.0000 - val_auc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1117/1117 [==============================] - 1s 844us/step - loss: 174775456497664.0000 - auc: 0.0000e+00 - val_loss: 217631848136704.0000 - val_auc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1117/1117 [==============================] - 1s 841us/step - loss: 174744015994880.0000 - auc: 0.0000e+00 - val_loss: 216000431652864.0000 - val_auc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1117/1117 [==============================] - 1s 875us/step - loss: 174723296133120.0000 - auc: 0.0000e+00 - val_loss: 216426438721536.0000 - val_auc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1117/1117 [==============================] - 1s 858us/step - loss: 174709001945088.0000 - auc: 0.0000e+00 - val_loss: 218164709294080.0000 - val_auc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1117/1117 [==============================] - 1s 830us/step - loss: 174698667180032.0000 - auc: 0.0000e+00 - val_loss: 218112515375104.0000 - val_auc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1117/1117 [==============================] - 1s 858us/step - loss: 174669592264704.0000 - auc: 0.0000e+00 - val_loss: 219530341122048.0000 - val_auc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1117/1117 [==============================] - 1s 826us/step - loss: 174689590706176.0000 - auc: 0.0000e+00 - val_loss: 216428804308992.0000 - val_auc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1117/1117 [==============================] - 1s 832us/step - loss: 174668686295040.0000 - auc: 0.0000e+00 - val_loss: 216680412217344.0000 - val_auc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1117/1117 [==============================] - 1s 856us/step - loss: 174651221213184.0000 - auc: 0.0000e+00 - val_loss: 215976490565632.0000 - val_auc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1117/1117 [==============================] - 1s 827us/step - loss: 174621508763648.0000 - auc: 0.0000e+00 - val_loss: 215795263078400.0000 - val_auc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1117/1117 [==============================] - 1s 846us/step - loss: 174578542313472.0000 - auc: 0.0000e+00 - val_loss: 216343458611200.0000 - val_auc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1117/1117 [==============================] - 1s 869us/step - loss: 174601141223424.0000 - auc: 0.0000e+00 - val_loss: 215786136272896.0000 - val_auc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1117/1117 [==============================] - 1s 901us/step - loss: 174539854053376.0000 - auc: 0.0000e+00 - val_loss: 216598270967808.0000 - val_auc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1117/1117 [==============================] - 1s 872us/step - loss: 174546682380288.0000 - auc: 0.0000e+00 - val_loss: 216984885133312.0000 - val_auc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1117/1117 [==============================] - 1s 895us/step - loss: 174539216519168.0000 - auc: 0.0000e+00 - val_loss: 215190243115008.0000 - val_auc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1117/1117 [==============================] - 1s 870us/step - loss: 174513463492608.0000 - auc: 0.0000e+00 - val_loss: 215363350429696.0000 - val_auc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1117/1117 [==============================] - 1s 855us/step - loss: 174473449832448.0000 - auc: 0.0000e+00 - val_loss: 216172767215616.0000 - val_auc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1117/1117 [==============================] - 1s 890us/step - loss: 174461236019200.0000 - auc: 0.0000e+00 - val_loss: 214464259424256.0000 - val_auc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1117/1117 [==============================] - 1s 870us/step - loss: 174435130671104.0000 - auc: 0.0000e+00 - val_loss: 215951979053056.0000 - val_auc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1117/1117 [==============================] - 1s 878us/step - loss: 174387131056128.0000 - auc: 0.0000e+00 - val_loss: 214637920387072.0000 - val_auc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1117/1117 [==============================] - 1s 885us/step - loss: 174419846627328.0000 - auc: 0.0000e+00 - val_loss: 214754371043328.0000 - val_auc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1117/1117 [==============================] - 1s 876us/step - loss: 174400670269440.0000 - auc: 0.0000e+00 - val_loss: 214301956636672.0000 - val_auc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1117/1117 [==============================] - 1s 863us/step - loss: 174383523954688.0000 - auc: 0.0000e+00 - val_loss: 214108465004544.0000 - val_auc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1117/1117 [==============================] - 1s 899us/step - loss: 174357133393920.0000 - auc: 0.0000e+00 - val_loss: 213455915188224.0000 - val_auc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x291545110>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128  \n",
    "epochs = 100    \n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396/1396 [==============================] - 1s 431us/step - loss: 182162649251840.0000 - auc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[182162649251840.0, 0.0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431/2431 [==============================] - 1s 403us/step\n"
     ]
    }
   ],
   "source": [
    "predicted_prices = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prices = predicted_prices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving predictions to a CSV file for Kaggle submission\n",
    "\n",
    "result_df = pd.DataFrame({'row ID': test_df['row ID'], 'price_doc': predicted_prices})\n",
    "result_df.to_csv('submission50_25253.csv', index=False)\n",
    "\n",
    "#result_df = pd.DataFrame({'row ID': test_df['row ID'], 'price_doc': predicted_prices.reshape(-1)})\n",
    "#result_df.to_csv('submission43_25253.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
